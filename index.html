<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="zh-CN">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="style.css" type="text/css" />
<link rel="shortcut icon" href="neu.png" type="image/x-icon" />
<title>刘骐瑞 - 个人主页</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category"></div>
<div class="menu-item"><a href="index.html" class="current">Menu</a></div>
<div class="menu-item"><a href="https://github.com/neuqrui">GitHub</a></div>
<div class="menu-item"><a href="https://www.luogu.com.cn/article/0ityj0pu7">LuoGu</a></div>
<div class="menu-item"><a href="https://scholar.google.com.hk/citations?user=phiH1v0AAAAJ&hl=zh-CN">Google Scholar</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>刘骐瑞</h1>
</div>

<table class="imgtable"><tr><td>
<a href="https://neuqrui.github.io/"><img src="photo.jpg" alt="刘骐瑞" width="131px" height="160px" /></a>&nbsp;</td>
<td align="left"><p>东北大学（秦皇岛）<br />计算机科学与技术专业</p>
<p>邮箱：<a href="mailto:202212056@stu.neuq.edu.cn">202212056@stu.neuq.edu.cn</a>, <a href="mailto:2845119457l@gmail.com">2845119457l@gmail.com</a></p>
<p>电话：15204369625</p>
<p>地址：吉林白城</p>
</td></tr></table>

<h2>教育背景</h2>
<ul>
<li><p><strong>东北大学（秦皇岛）</strong> - 计算机科学与技术 本科</p>
<p>计算机与通信工程学院</p>
<p><strong>专业排名</strong>: 15 / 191 (7.8%)</p>
<p><strong>主要课程</strong>: 高等数学 (99)、人工智能导论 (98)、C++程序设计 (97)、算法设计与分析 (97)、数据结构 (96)</p>
<p><strong>获奖情况</strong>: 国家励志奖学金、校综合一等奖学金 (3次)、三好学生标兵、三好学生、优秀共青团员</p>
</li>
</ul>

<h2>科研基础</h2>
<ul>
<li>围绕 Deep Reinforcement Learning、Reinforcement Learning、Decision Making 等领域，以学生第一作者身份发表 1 篇 JCR 2 区 SCI 期刊论文。</li>
<li>熟悉 PyTorch 框架，理解并实现了 Transformer、Action Chunking with Transformer 等经典模型。</li>
<li>熟悉 Overleaf 写作，擅长英文论文撰写；擅长使用 draw.io 绘制论文图表。</li>
<li>熟悉 PPO、GRPO、DQN 等强化学习算法，以及大模型基础知识和 RLHF 过程。</li>
</ul>

<h2>科研论文</h2>
<ul>
<p><strong>BoxesZero: An Efficient and Computationally Frugal Dots-and-Boxes Agent</strong> (JCR 2 区期刊，共同一作)</p>
    <p>提出了一种高效的 Dots-and-Boxes 智能体，融合了多种创新技术，实验结果表明模型在训练 40 小时后达到了 SOTA 水平。</p>
    <p><strong>负责内容</strong>: 提出并实现整个 BoxesZero 框架；参与论文编写和图表绘制。</p>

    <!-- Markdown风格的解释文本 -->
    <h3>论文讲解</h3>
    <p>在这篇论文中，我们详细介绍了BoxesZero的主要创新技术：</p>
<ol>
    <li>Two-stage self-play：在模型初期我们只采集残局的数据(这样的数据具有真实的回报信号，进行预训练（这个过程相当于价值网络在学习对丰富的局面的价值评估）,<br>
        模型逐步前进，通过观察可以看到局的状态价值从0逐渐接近1，表明价值网络在提升，可以辨别出局面的输赢。)在self-play第一阶段收集的数据为不同局面经过MCTS<br>
        搜索后根节点的状态价值Q（S）训练价值网络以及MCTS返回的策略pi。 此阶段由于价值信号较为丰富，价值网络收敛较快，因此称为Strengthen value network. <br>
        在self-play的第二阶段采用0.25Q+0.75z的混合价值信号方法。第二阶段相当于对策略网络进行微调，预训练阶段的学习任务相当于对局面进行认知判断，但是最终的目<br>
        的只要能赢就可以，因此采用输赢z进行微调，在此过程中可以观察到策略网络的收敛。因此为Strengthen policy network。
    <p><img src="two_stage_play.jpg" alt="Two stage self-play" class="clickable-img" style="width:1000%;max-width:1000px;"></p>
</li>
    <li>Data augmentation: 针对预训练阶段的数据量少的问题提出了一种Data augmentation的方法，在alphazero的做法中mcts搜索结束后，就将整个树都丢弃了，我<br>
        们考虑在树中抽取一些高质量的节点，进行训练。具体方案可以根据节点搜索次数N，选择Top k个节点(K取3~5)，或者根据固定的N和Q的约束条件，因为N，Q绝对值越大，<br>
        可以再一定程度上说明这个节点质量高。最终经过旋转对称再一次数据增强。最后的数据量对比图。左侧为Boxeszero在self-play阶段不同的iteration和不同step下<br>
        对应的数据量。
    <p><img src="dataaug.jpg" alt="Data augmentation" class="clickable-img" style="width:1000%;max-width:1000px;"></p>
    </li>

    <li>Extended endgame theorem: 针对点格棋本身的局面特性可以找出一些必走策略，在MCTS中遇到这种必走节点可以只纵向展开，不横向展开。经过剪枝的MCTS的树形<br>
        更加的身，也就意味着奖励信号更加充足。 在已有的残局定理基础上证明了存在含有1-chain和2-chain的局面。<br>
    <p><img src="endgame.jpg" alt="Data augmentation" class="clickable-img" style="width:1000%;max-width:1000px;"></p>
    </li>
    <li>Network Structure: 我们的网络结构主要分为三个部分：<br>
        1.input层，对棋盘进行初步手工特征提取，将一个H*W的棋盘变为（3，H，W）3个channel，第一个channel是分数差，第二个channel边的位置为1，无边为0。第三<br>
        个channel是有格子的位置为1，无为0。这样提取不会损失棋盘原始的任何信息。<br>
        2.第二层是特征提取层主要采用N块类似残差块的网络结构。负责对局面进行更深层次的特征提取。<br>
        3.第三层分为Value head和policy head。输出分别为当前棋盘的state value，和策略pi。<br>
    <p><img src="net.jpg" alt="Data augmentation" class="clickable-img" style="width:1000%;max-width:1000px;"></p>
    </li>

</ol>


    <!-- Markdown风格的图片插入 -->


    <p> 通过实验，我们证明了BoxesZero在多个指标上都优于现有的方法...</p>
<li><p><strong>融合轻量化哈希与 Minimax 的 MCTS 算法</strong> (北大核心期刊，在校，共同一作)</p>
<p>提出了一种轻量化哈希技术，将点格栅的局面特征提取为整数，哈希命中率提高了近 1 倍。</p>
<p><strong>负责内容</strong>: 提出 idea 并编写代码实现；参与论文核心内容撰写。</p>
</li>
</ul>

<h2>竞赛经历</h2>
<ul>
<li>中国大学生计算机博弈大赛点格棋赛道 (队长)，全国一等奖 (冠军), 北京大学 Botzone 在线程序对抗平台第一名</li>
<li>中国大学生计算机博弈大赛五子棋赛道，全国一等奖</li>
<!--<li>北京大学 Botzone 在线程序对抗平台第一名</li>-->
<li>机器人开发者大赛 (RAICOM) 全国总决赛“编程技能赛”，全国三等奖</li>
<li>中国大学生程序设计竞赛国家桂林站 (CCPC) 优胜奖</li>
<li>全国大学生数学竞赛二等奖</li>
</ul>

<div id="footer">
<div id="footer-text">
&copy; 2025 刘琪瑞. 保留所有权利.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
